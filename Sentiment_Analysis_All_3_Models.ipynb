{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3I25YBuUrrFP",
        "outputId": "bb5c3d4b-99ae-4e38-bea2-98eadfb3f698"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 82,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "XNnml5xHeDQc"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "e9pVsBE9XeUY"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "nm_UwcDYXidW"
      },
      "outputs": [],
      "source": [
        "df=pd.read_csv('/content/drive/MyDrive/Reviews/Reviews1.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "IzejEB1yXpkP"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "JARQtgEcZkVz"
      },
      "outputs": [],
      "source": [
        "!pip install contractions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "elVoZ_0DiMEq"
      },
      "outputs": [],
      "source": [
        "!pip install fasttext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "laoOl5IUZ35E"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import string\n",
        "import fasttext\n",
        "import contractions\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "plt.xticks(rotation=70)\n",
        "pd.options.mode.chained_assignment = None\n",
        "pd.set_option('display.max_colwidth', 100)\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "CfpRVkAKZ6IM"
      },
      "outputs": [],
      "source": [
        "for col in df.columns:\n",
        "    print(col, df[col].isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "pYaHKw-ZbxcR"
      },
      "outputs": [],
      "source": [
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "nhNvt6akb6Ab"
      },
      "outputs": [],
      "source": [
        "rws = df.loc[:, ['ProductId','Summary','Text']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "JcacWYMrc15z"
      },
      "outputs": [],
      "source": [
        "#Expanding Contradictions\n",
        "rws['Summary'] = rws['Text'].apply(lambda x: [contractions.fix(word) for word in x.split()])\n",
        "rws.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Dcwby2wpehMR"
      },
      "outputs": [],
      "source": [
        "rws['review_description_str'] = [' '.join(map(str, l)) for l in rws['Summary']]\n",
        "rws.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8oKL6TGLfAru"
      },
      "outputs": [],
      "source": [
        "# Define text preprocessing function\n",
        "def clean(text):\n",
        "    import html\n",
        "    import string\n",
        "    import nltk\n",
        "    nltk.download('wordnet')\n",
        "\n",
        "    line = html.unescape(text)\n",
        "    line = line.replace(\"can't\", 'can not')\n",
        "    line = line.replace(\"n't\", \" not\")\n",
        "    # Pad punctuations with white spaces\n",
        "    pad_punct = str.maketrans({key: \" {0} \".format(key) for key in string.punctuation})\n",
        "    line = line.translate(pad_punct)\n",
        "    line = line.lower()\n",
        "    line = line.split()\n",
        "    lemmatizer = nltk.WordNetLemmatizer()\n",
        "    line = [lemmatizer.lemmatize(t) for t in line]\n",
        "\n",
        "    # Negation handling\n",
        "    # Add \"not_\" prefix to words behind \"not\", or \"no\" until the end of the sentence\n",
        "    tokens = []\n",
        "    negated = False\n",
        "    for t in line:\n",
        "        if t in ['not', 'no']:\n",
        "            negated = not negated\n",
        "        elif t in string.punctuation or not t.isalpha():\n",
        "            negated = False\n",
        "        else:\n",
        "            tokens.append('not_' + t if negated else t)\n",
        "\n",
        "    invalidChars = str(string.punctuation.replace(\"_\", \"\"))\n",
        "    bi_tokens = list(nltk.bigrams(line))\n",
        "    bi_tokens = list(map('_'.join, bi_tokens))\n",
        "    bi_tokens = [i for i in bi_tokens if all(j not in invalidChars for j in i)]\n",
        "    tri_tokens = list(nltk.trigrams(line))\n",
        "    tri_tokens = list(map('_'.join, tri_tokens))\n",
        "    tri_tokens = [i for i in tri_tokens if all(j not in invalidChars for j in i)]\n",
        "    tokens = tokens + bi_tokens + tri_tokens\n",
        "\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "vzz9xa2VmnJ6"
      },
      "outputs": [],
      "source": [
        "example = clean(\"I don't liked the food\")\n",
        "print(example)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4Oy3X6g7nOQd"
      },
      "outputs": [],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "24-4tfNSoSea"
      },
      "outputs": [],
      "source": [
        "from pyspark.shell import spark\n",
        "spark.conf.set('spark.sql.shuffle.partitions', '8')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Tp51-LgFrUjd"
      },
      "outputs": [],
      "source": [
        "# importing packages\n",
        "import pandas as pd\n",
        "\n",
        "# create data\n",
        "df1 = pd.DataFrame(df)\n",
        "\n",
        "# view data\n",
        "display(df1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "sVSIQUOvvFkO"
      },
      "outputs": [],
      "source": [
        "# Using groupby() and count()\n",
        "df2 = df1.groupby(['Score'])['Score'].count()\n",
        "df2.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "hPe2Rwe5JxAI"
      },
      "outputs": [],
      "source": [
        "!pip install -U sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "WPjooGbmKKiG"
      },
      "outputs": [],
      "source": [
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "YY5gKJIdPgol"
      },
      "outputs": [],
      "source": [
        "import codecs\n",
        "import csv\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "stop_words = set(stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "cNa_otH7QsUx"
      },
      "outputs": [],
      "source": [
        "from spacy.lang.en import English\n",
        "\n",
        "# Load English tokenizer, tagger, parser, NER and word vectors\n",
        "nlp = English()\n",
        "\n",
        "text = \"\"\"He determined to drop his litigation with the monastry, and relinguish his claims to the wood-cuting and\n",
        "fishery rihgts at once. He was the more ready to do this becuase the rights had become much less valuable, and he had\n",
        "indeed the vaguest idea where the wood and river in question were.\"\"\"\n",
        "\n",
        "#  \"nlp\" Object is used to create documents with linguistic annotations.\n",
        "my_doc = nlp(text)\n",
        "\n",
        "# Create list of word tokens\n",
        "token_list = []\n",
        "for token in my_doc:\n",
        "    token_list.append(token.text)\n",
        "\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "\n",
        "# Create list of word tokens after removing stopwords\n",
        "filtered_sentence =[]\n",
        "\n",
        "for word in token_list:\n",
        "    lexeme = nlp.vocab[word]\n",
        "    if lexeme.is_stop == False:\n",
        "        filtered_sentence.append(word)\n",
        "print(token_list)\n",
        "print(filtered_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "DBHMPhqCkdjj"
      },
      "outputs": [],
      "source": [
        "!pip3 install sentence_transformers\n",
        "#!pip3 install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Q1EmB3yKkgQA"
      },
      "outputs": [],
      "source": [
        "!pip install -U spacy\n",
        "!python -m spacy download en_core_web_lg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "484cVhF_klqz"
      },
      "outputs": [],
      "source": [
        "print(df.iloc[123]['Text'])\n",
        "print(df.iloc[1243]['Text'])\n",
        "print(df.iloc[23]['Text'])\n",
        "print(df.iloc[389]['Text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6gJ3zMPwmORb"
      },
      "outputs": [],
      "source": [
        "# starts 4-5: Positive(1), stars 1-2: Negative(3), stars 3: Neutral(2)\n",
        "def map_sentiment(rating):\n",
        "    if(int(rating)==3):\n",
        "        return 2\n",
        "    elif(int(rating)<3):\n",
        "        return 3\n",
        "    else:\n",
        "        return 1\n",
        "\n",
        "df['Score']\n",
        "review_sentiments=[map_sentiment(s) for s in df['Score']]\n",
        "df['sentiments']=review_sentiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "IUN-xvDamm9L"
      },
      "outputs": [],
      "source": [
        "df.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "QWVE0HVGm0IC"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "m2R9wQ6EnJiL"
      },
      "outputs": [],
      "source": [
        "df['Date']=pd.to_datetime(df['Date'])\n",
        "df.value_counts(['ProductId'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXmgtu0Vxrob"
      },
      "source": [
        "Most Reviewed Brand"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1HvuRYJEofKR"
      },
      "outputs": [],
      "source": [
        "# Get most review business ids\n",
        "ids=df.value_counts(['ProductId'])\n",
        "ids=[list(ids.index[i])[0] for i in range(len(ids))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "jHvzb_5QpTVB"
      },
      "outputs": [],
      "source": [
        "data0=df[df['ProductId']==ids[0]]\n",
        "data1=df[df['ProductId']==ids[1]]\n",
        "data2=df[df['ProductId']==ids[2]]\n",
        "data3=df[df['ProductId']==ids[3]]\n",
        "data4=df[df['ProductId']==ids[4]]\n",
        "data=pd.concat([data0,data1,data2,data3,data4])\n",
        "len(data)\n",
        "data.to_csv('review_business_merge.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "nknEQEWCp2VN"
      },
      "outputs": [],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "MeHAghtkqVE7"
      },
      "outputs": [],
      "source": [
        "data_most_reviewed_store=df[df['ProductId']=='B007JFMH8M'].sort_values(by=['Time'])\n",
        "data_most_reviewed_store.shape\n",
        "business_rating=data_most_reviewed_store.iloc[0]['Score']\n",
        "print('Overall rating of the store:',business_rating)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "MTmRUKDsrfXE"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import plotly as py\n",
        "import cufflinks\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "from branca.element import Figure\n",
        "import folium\n",
        "import matplotlib.pyplot as pPlot\n",
        "import numpy as npy\n",
        "from PIL import Image\n",
        "from IPython.display import Image as img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "qQWucYC1rGOq"
      },
      "outputs": [],
      "source": [
        "# Visualize the ratings\n",
        "\n",
        "rating_5=len(df[df['Score']==5])\n",
        "rating_4=len(df[df['Score']==4])\n",
        "rating_3=len(df[df['Score']==3])\n",
        "rating_2=len(df[df['Score']==2])\n",
        "rating_1=len(df[df['Score']==1])\n",
        "def getStarRatings():\n",
        "    fig = go.Figure()\n",
        "    fig.add_trace(go.Bar(\n",
        "            y=[1],\n",
        "            x=[rating_1],\n",
        "            name='1',\n",
        "            orientation='h',marker=dict(\n",
        "        color='rgb(255, 51, 51)'\n",
        "    )))\n",
        "    fig.add_trace(go.Bar(\n",
        "            y=[2],\n",
        "            name='2',\n",
        "            x=[rating_2],\n",
        "            orientation='h',marker=dict(\n",
        "        color='rgb(255, 92, 51)'\n",
        "    )))\n",
        "    fig.add_trace(go.Bar(\n",
        "            y=[3],\n",
        "            name='3',\n",
        "            x=[rating_3],\n",
        "            orientation='h',marker=dict(\n",
        "        color='rgb(255, 255, 77)'\n",
        "    )))\n",
        "    fig.add_trace(go.Bar(\n",
        "            y=[4],\n",
        "            name='4',\n",
        "            x=[rating_4],\n",
        "            orientation='h',marker=dict(\n",
        "        color='rgb(77, 255, 166)'\n",
        "    )))\n",
        "    fig.add_trace(go.Bar(\n",
        "            y=[5],\n",
        "            name='5',\n",
        "            x=[rating_5],\n",
        "            orientation='h',marker=dict(\n",
        "        color='rgb(166, 255, 77)'\n",
        "\n",
        "     ))),\n",
        "    fig.update_layout(\n",
        "     autosize=False,\n",
        "     width=500,\n",
        "     height=500,\n",
        "     title='Distribution of Review Ratings'\n",
        "    )\n",
        "    return fig\n",
        "getStarRatings()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "dxE8zeXQrorw"
      },
      "outputs": [],
      "source": [
        "print('No of positive Review:',str(len(df[df['sentiments']==1])))\n",
        "print('No of negative Review:',str(len(df[df['sentiments']==3])))\n",
        "print('No of neutral Review:',str(len(df[df['sentiments']==2])))\n",
        "sentimments_dict={3:'Negative',2:'Neutral',1:'Positive'}\n",
        "sentiment_names=[sentimments_dict[int(i)] for i in data_most_reviewed_store['sentiments'].values]\n",
        "data_most_reviewed_store['sentiment_name']=sentiment_names\n",
        "fig = px.pie(data_most_reviewed_store, values='sentiments', names='sentiment_name',color='sentiment_name',color_discrete_map={'Neutral':'yellow','Negative':'cyan','Positive':'green'})\n",
        "fig.update_layout(\n",
        "     autosize=False,\n",
        "     title='Distribution of Review Sentiments'\n",
        "    )\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "oBNKNnkgtOUX"
      },
      "outputs": [],
      "source": [
        "date_str=[en.strftime('%Y') for  en in data_most_reviewed_store['Date']]\n",
        "data_most_reviewed_store['Date_year']=date_str\n",
        "# Sum the number of reviews per year\n",
        "data_most_reviewed_store_timeseries_pos=data_most_reviewed_store[data_most_reviewed_store['sentiments']==1].groupby('Date_year')['Date_year','sentiments'].sum()\n",
        "data_most_reviewed_store_timeseries_neg=data_most_reviewed_store[data_most_reviewed_store['sentiments']==3].groupby('Date_year')['Date_year','sentiments'].sum()\n",
        "data_most_reviewed_store_timeseries_neu=data_most_reviewed_store[data_most_reviewed_store['sentiments']==2].groupby('Date_year')['Date_year','sentiments'].sum()\n",
        "data_most_reviewed_store_timeseries_pos['Rating']=['Positive' for i in range(len(data_most_reviewed_store_timeseries_pos))]\n",
        "data_most_reviewed_store_timeseries_neg['Rating']=['Negative' for i in range(len(data_most_reviewed_store_timeseries_neg))]\n",
        "data_most_reviewed_store_timeseries_neu['Rating']=['Neutral' for i in range(len(data_most_reviewed_store_timeseries_neu))]\n",
        "data_most_reviewed_store_timeseries=pd.concat([data_most_reviewed_store_timeseries_pos,data_most_reviewed_store_timeseries_neg])\n",
        "data_most_reviewed_store_timeseries=pd.concat([data_most_reviewed_store_timeseries,data_most_reviewed_store_timeseries_neu])\n",
        "# No of reviews per year\n",
        "data_most_reviewed_store_timeseries['Year']=data_most_reviewed_store_timeseries.index\n",
        "data_most_reviewed_store_timeseries.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ve4BEil3O5zc"
      },
      "outputs": [],
      "source": [
        "from plotly.subplots import make_subplots\n",
        "fig = go.Figure()\n",
        "\n",
        "fig.add_trace(go.Scatter(x=list(data_most_reviewed_store_timeseries.index),\n",
        "                         y=data_most_reviewed_store_timeseries_pos['sentiments'],\n",
        "                         mode='lines',\n",
        "                         name='positive',\n",
        "                         line=dict(color='rgb(0,245,153)', width=1)))\n",
        "fig.add_trace(go.Scatter(x=list(data_most_reviewed_store_timeseries.index),\n",
        "                         y=data_most_reviewed_store_timeseries_neg['sentiments'],\n",
        "                         mode='lines',\n",
        "                         name='negative',\n",
        "                         line=dict(color='rgb(255, 102, 102)', width=1)))\n",
        "fig.add_trace(go.Scatter(x=list(data_most_reviewed_store_timeseries.index),\n",
        "                         y=data_most_reviewed_store_timeseries_neu['sentiments'],\n",
        "                         mode='lines',\n",
        "                         name='neutral',\n",
        "                         line=dict(color='rgb(102, 102, 255)', width=1)))\n",
        "fig.update_layout(\n",
        "     autosize=False,\n",
        "     title='Trend of sentiments over time'\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "kje1oDV_PAzp"
      },
      "outputs": [],
      "source": [
        "fig = px.bar(data_most_reviewed_store_timeseries.tail(30), x='Year', y='sentiments',color='Rating')\n",
        "fig.update_layout(\n",
        "     autosize=False,\n",
        "     title='Trend of sentiments over time - Bar Graph'\n",
        "    )\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "xK8xTu3mPKMR"
      },
      "outputs": [],
      "source": [
        "corpus_pos=\" \".join(list(data_most_reviewed_store[data_most_reviewed_store['sentiments']==1]['Text']))\n",
        "corpus_neg=\" \".join(list(data_most_reviewed_store[data_most_reviewed_store['sentiments']==2]['Text']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "LvCQ-7j7PX8Z"
      },
      "outputs": [],
      "source": [
        "STOPWORDS.add(\"doughnut\")\n",
        "STOPWORDS.add(\"donuts\")\n",
        "STOPWORDS.add(\"voodoo\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_GQuCYo_Pa3K"
      },
      "outputs": [],
      "source": [
        "def create_word_cloud(string):\n",
        "    cloud = WordCloud(background_color = \"white\", max_words = 15000, stopwords = set(STOPWORDS))\n",
        "    cloud.generate(string)\n",
        "    cloud.to_file(\"wordCloud.png\")\n",
        "\n",
        "# Postitive\n",
        "create_word_cloud(corpus_pos.lower())\n",
        "img('/content/wordCloud.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "vuFbdF-EPhoJ"
      },
      "outputs": [],
      "source": [
        "create_word_cloud(corpus_neg.lower())\n",
        "img('/content/wordCloud.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5XtNqn0qPoqD"
      },
      "outputs": [],
      "source": [
        "# Import Necessary Libraries\n",
        "import re\n",
        "import nltk\n",
        "import numpy as np\n",
        "import itertools\n",
        "import pickle\n",
        "import spacy\n",
        "from tqdm.notebook import tqdm as tqdm\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from sentence_transformers import SentenceTransformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ya5-UC2JPsuw"
      },
      "outputs": [],
      "source": [
        "# Stopwards\n",
        "stopwords = list(set(stopwords.words(\"english\")))\n",
        "stopwords+=['voodoo','doughnuts','doughnut']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "l-SJrk6vPwth"
      },
      "outputs": [],
      "source": [
        "# Filtering the dataset based on Review Sentiments\n",
        "positive_reviews=data_most_reviewed_store[data_most_reviewed_store['sentiments']==1]\n",
        "negative_reviews=data_most_reviewed_store[data_most_reviewed_store['sentiments']==3]\n",
        "neutral_reviews=data_most_reviewed_store[data_most_reviewed_store['sentiments']==2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "IBlhf1hCP1SD"
      },
      "outputs": [],
      "source": [
        "# Method to preprocess the data\n",
        "def preprocess(data):\n",
        "    # Preprocessing Texts\n",
        "    preprocessed_texts = []\n",
        "    lem = WordNetLemmatizer()\n",
        "    # Cleaing the data, removing stopwords\n",
        "    for sent in tqdm(data):\n",
        "        sent = sent.replace('\\\\r', ' ')\n",
        "        sent = sent.replace('\\\\\"', ' ')\n",
        "        sent = sent.replace('\\\\n', ' ')\n",
        "        sent = re.sub('[^A-Za-z ]+', ' ', sent)\n",
        "        # lemmatizing\n",
        "        sent=' '.join(lem.lemmatize(word) for word in sent.split() if word not in stopwords )\n",
        "        preprocessed_texts.append(sent.lower().strip())\n",
        "    return preprocessed_texts\n",
        "\n",
        "preprocessed_texts_neg=preprocess(negative_reviews['Text'].values)\n",
        "preprocessed_texts_pos=preprocess(positive_reviews['Text'].values)\n",
        "preprocessed_texts_neu=preprocess(neutral_reviews['Text'].values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "yyhRKz8fQE3R"
      },
      "outputs": [],
      "source": [
        "preprocessed_texts_pos[67]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "H0I8mjp1Q6mi"
      },
      "outputs": [],
      "source": [
        "# Import Necessary Libraries\n",
        "import re\n",
        "import nltk\n",
        "import numpy as np\n",
        "import itertools\n",
        "import spacy\n",
        "from tqdm.notebook import tqdm as tqdm\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "#nltk.download('stopwords')\n",
        "#nltk.download('punkt')\n",
        "#nltk.download('wordnet')\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "class CustomerReviewKeywords1:\n",
        "    # Method to preprocess the data\n",
        "    stopwords_ = list(set(stopwords.words(\"english\")))\n",
        "    stopwords_+=['voodoo','doughnuts','doughnut','voodoodoughnut']\n",
        "    def preprocess(self,data,stopwords):\n",
        "        # Preprocessing Texts\n",
        "        preprocessed_texts = []\n",
        "        lem = WordNetLemmatizer()\n",
        "        # Cleaing the data, removing stopwords\n",
        "        for sent in data:\n",
        "            sent = sent.replace('\\\\r', ' ')\n",
        "            sent = sent.replace('\\\\\"', ' ')\n",
        "            sent = sent.replace('\\\\n', ' ')\n",
        "            sent = re.sub('[^A-Za-z ]+', ' ', sent)\n",
        "            # lemmatizing\n",
        "            sent=' '.join(word for word in sent.split() if word not in stopwords)\n",
        "            preprocessed_texts.append(sent.lower().strip())\n",
        "        return preprocessed_texts\n",
        "\n",
        "    def max_sum_sim(self,doc_embedding, candidate_embeddings, candidates, top_n, nr_candidates):\n",
        "        # Calculate distances and extract keywords\n",
        "        distances = cosine_similarity(doc_embedding, candidate_embeddings)\n",
        "        distances_candidates = cosine_similarity(candidate_embeddings,\n",
        "                                            candidate_embeddings)\n",
        "\n",
        "        # Get top_n words as candidates based on cosine similarity\n",
        "        words_idx = list(distances.argsort()[0][-nr_candidates:])\n",
        "        words_vals = [candidates[index] for index in words_idx]\n",
        "        distances_candidates = distances_candidates[np.ix_(words_idx, words_idx)]\n",
        "\n",
        "        # Calculate the combination of words that are the least similar to each other\n",
        "        min_sim = np.inf\n",
        "        candidate = None\n",
        "        for combination in itertools.combinations(range(len(words_idx)), top_n):\n",
        "            sim = sum([distances_candidates[i][j] for i in combination for j in combination if i != j])\n",
        "            if sim < min_sim:\n",
        "                candidate = combination\n",
        "                min_sim = sim\n",
        "        return [words_vals[idx] for idx in candidate]\n",
        "\n",
        "    # Diversify the keywords using max sum similarity, higher the value of nr_candidates higher the diversity\n",
        "    def extract_keywords_bert_diverse(self,doc,stopwords,top_n=10,nr_candidates=20):\n",
        "        n_gram_range = (1,1)\n",
        "        # Extract candidate words/phrases using count vectorizer (TF-IDF Scores)\n",
        "        count = CountVectorizer(ngram_range=n_gram_range, stop_words=stopwords).fit([doc])\n",
        "        candidates = count.get_feature_names()\n",
        "        # Embeddings of the document using Bert\n",
        "        model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
        "        doc_embedding = model.encode([doc])\n",
        "        candidate_embeddings = model.encode(candidates)\n",
        "        keywords=self.max_sum_sim(doc_embedding, candidate_embeddings, candidates, top_n, nr_candidates)\n",
        "        return keywords\n",
        "\n",
        "    # Method to get the trending keywords\n",
        "    def get_trending_keywords(self,data_most_reviewed_store,num_keywords=5):\n",
        "        # Stopwards\n",
        "        stopwords_ = list(set(stopwords.words(\"english\")))\n",
        "        stopwords_+=['voodoo','doughnuts','doughnut','voodoodoughnut','donut','donuts']\n",
        "        # Filtering the dataset based on Review Sentiments\n",
        "        positive_reviews=data_most_reviewed_store[data_most_reviewed_store['sentiments']==1]\n",
        "        negative_reviews=data_most_reviewed_store[data_most_reviewed_store['sentiments']==3]\n",
        "        neutral_reviews=data_most_reviewed_store[data_most_reviewed_store['sentiments']==2]\n",
        "        preprocessed_texts_neg=self.preprocess(negative_reviews['Text'].values,stopwords_)\n",
        "        preprocessed_texts_pos=self.preprocess(positive_reviews['Text'].values,stopwords_)\n",
        "        preprocessed_texts_neu=self.preprocess(neutral_reviews['Text'].values,stopwords_)\n",
        "        keywords={}\n",
        "        corpus=' '.join(preprocessed_texts_pos[-500::])\n",
        "        keywords['positive']=self.extract_keywords_bert_diverse(corpus,stopwords_,num_keywords)\n",
        "        corpus=' '.join(preprocessed_texts_neg[-500::])\n",
        "        keywords['negative']=self.extract_keywords_bert_diverse(corpus,stopwords_,num_keywords)\n",
        "        return keywords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Ja9hPf9YRDDz"
      },
      "outputs": [],
      "source": [
        "ck=CustomerReviewKeywords1()\n",
        "data_most_reviewed_store=df[df['ProductId']==ids[5]].sort_values(by=['Date'])\n",
        "kw=ck.get_trending_keywords(data_most_reviewed_store,5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "45KmyRR5SHbC"
      },
      "outputs": [],
      "source": [
        "df1=pd.DataFrame(columns=['id','pos_keywords','neg_keywords'])\n",
        "review_ids=[]\n",
        "pos_keywords=[]\n",
        "neg_keywords=[]\n",
        "\n",
        "for i in range(10):\n",
        "    ck=CustomerReviewKeywords1()\n",
        "    data_most_reviewed_store=df[df['ProductId']==ids[int(i)]].sort_values(by=['Date'])\n",
        "    kw=ck.get_trending_keywords(data_most_reviewed_store,num_keywords=5)\n",
        "    review_ids.append(ids[int(i)])\n",
        "    pos_keywords.append(kw['positive'])\n",
        "    neg_keywords.append(kw['negative'])\n",
        "df1['id']=review_ids\n",
        "df1['pos_keywords']=pos_keywords\n",
        "df1['neg_keywords']=neg_keywords\n",
        "df1.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kEfd4NeZ0cUN"
      },
      "outputs": [],
      "source": [
        "data_most_reviewed_store.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6s3HnSKJ0tXW"
      },
      "outputs": [],
      "source": [
        "def recent_reviews(review_business_data_merged,id):\n",
        "    data_most_reviewed_store=review_business_data_merged[review_business_data_merged['ProductId']==id].sort_values(by=['Date'],ascending=False)\n",
        "    sentimments_dict={3:'Negative',2:'Neutral',1:'Positive'}\n",
        "    sentiment_names=[sentimments_dict[int(i)] for i in data_most_reviewed_store['sentiments'].values]\n",
        "    data_most_reviewed_store['sentiment']=sentiment_names\n",
        "    data_most_reviewed_store=data_most_reviewed_store.drop(columns=['ProductId','UserId','ProfileName'])\n",
        "    data_most_reviewed_store=data_most_reviewed_store[['Text','sentiment','Date','Score']]\n",
        "    data_most_reviewed_store=data_most_reviewed_store.rename(columns={'Score':'Rating','Text':'Review'})\n",
        "    return data_most_reviewed_store\n",
        "\n",
        "recent_reviews(df,ids[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xwFenDYdSqBy"
      },
      "outputs": [],
      "source": [
        "import pyspark\n",
        "spark.conf.set('spark.sql.shuffle.partitions', '8')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7RwBFczYSx6r"
      },
      "outputs": [],
      "source": [
        "# load .csv data\n",
        "kindle_csv = spark.read.format(\"csv\").option('header','true').load(\"/content/drive/MyDrive/Reviews/Reviews1.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VMU9ukhmTBor"
      },
      "outputs": [],
      "source": [
        "kindle_csv.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xE9ql02qTvd_"
      },
      "outputs": [],
      "source": [
        "#negative (label = 1)  positive (label = 0)\n",
        "kindle_csv.createOrReplaceTempView('kindle_view')\n",
        "\n",
        "data_csv = spark.sql('''\n",
        "  SELECT CASE WHEN Score<4 THEN 1\n",
        "          ELSE 0\n",
        "          END as label,\n",
        "          Text as text\n",
        "  FROM kindle_view\n",
        "  WHERE length(text)>2''')\n",
        "\n",
        "data_csv.groupBy('label').count().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f0HnjM-RV_Nr"
      },
      "outputs": [],
      "source": [
        "# Sampling data\n",
        "pos = data_csv.where('label=0').sample(False, 0.05, seed=1220)\n",
        "neg = data_csv.where('label=1').sample(False, 0.25, seed=1220)\n",
        "data = pos.union(neg)\n",
        "data.groupBy('label').count().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFCGMVdaWHXy"
      },
      "outputs": [],
      "source": [
        "# Negative reviews are on average longer than the positive reviews, but not significantly longer\n",
        "from pyspark.sql.functions import length\n",
        "data.withColumn('review_length', length('text')).groupBy('label').avg('review_length').show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "41xtFT3PWdtm"
      },
      "outputs": [],
      "source": [
        "# Perform data preprocessing\n",
        "from pyspark.sql.functions import udf, col, size\n",
        "from pyspark.sql.types import ArrayType, StringType\n",
        "clean_udf = udf(clean, ArrayType(StringType()))\n",
        "data_tokens = data.withColumn('tokens', clean_udf(col('text')))\n",
        "data_tokens.show(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-dTV7KCpWl8N"
      },
      "outputs": [],
      "source": [
        "# Split data to 70% for training and 30% for testing\n",
        "training, testing = data_tokens.randomSplit([0.7,0.3], seed=1220)\n",
        "training.groupBy('label').count().show(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YQF7c0XjWq5N"
      },
      "outputs": [],
      "source": [
        "training.cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-8ZeG2vXPlC"
      },
      "source": [
        "Naive Bayes Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_VdY3Is4XMyf"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import CountVectorizer, IDF\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "count_vec = CountVectorizer(inputCol='tokens', outputCol='c_vec', minDF=5.0)\n",
        "idf = IDF(inputCol=\"c_vec\", outputCol=\"features\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "bqNO-COfXchZ"
      },
      "outputs": [],
      "source": [
        "# Naive Bayes model\n",
        "from pyspark.ml.classification import NaiveBayes\n",
        "nb = NaiveBayes()\n",
        "\n",
        "pipeline_nb = Pipeline(stages=[count_vec, idf, nb])\n",
        "\n",
        "model_nb = pipeline_nb.fit(training)\n",
        "test_nb = model_nb.transform(testing)\n",
        "test_nb.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ytpKuV86fTK"
      },
      "source": [
        "Naive Bayes Model Using default parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "DIR0pYcy6cJl",
        "outputId": "7107175f-c1c0-46e6-dace-c5317a2717cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ROC of the NB model: 0.8636859575655796\n"
          ]
        }
      ],
      "source": [
        "# Naive Bayes model ROC\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "roc_nb_eval = BinaryClassificationEvaluator(rawPredictionCol='prediction', labelCol='label')\n",
        "roc_nb = roc_nb_eval.evaluate(test_nb)\n",
        "print(\"ROC of the NB model: {}\".format(roc_nb))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "qY_TNA9s6pn7",
        "outputId": "475d9754-59d8-4302-e27f-155e10fb2261"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of the NB model: 0.862066846891128\n"
          ]
        }
      ],
      "source": [
        "# Naive Bayes model accuracy\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "acc_nb_eval = MulticlassClassificationEvaluator(metricName='accuracy')\n",
        "acc_nb = acc_nb_eval.evaluate(test_nb)\n",
        "print(\"Accuracy of the NB model: {}\".format(acc_nb))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9yLJnvEXKf_"
      },
      "source": [
        "Logistic Regressions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "IMEIjjWIXNdI"
      },
      "outputs": [],
      "source": [
        "# Logistic Regression model\n",
        "from pyspark.ml.feature import CountVectorizer, IDF\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "lgr = LogisticRegression(maxIter=5)\n",
        "pipeline_lgr = Pipeline(stages=[count_vec, idf, lgr])\n",
        "\n",
        "model_lgr = pipeline_lgr.fit(training)\n",
        "test_lgr = model_lgr.transform(testing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Ic41LaXbXg4p",
        "outputId": "37ea7ea3-c099-4b85-9853-8bf466a7db9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ROC of the model: 0.8801258533761804\n"
          ]
        }
      ],
      "source": [
        "# Logistic Regression model ROC\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "roc_lgr_eval = BinaryClassificationEvaluator(rawPredictionCol='prediction', labelCol='label')\n",
        "roc_lgr = roc_lgr_eval.evaluate(test_lgr)\n",
        "print(\"ROC of the model: {}\".format(roc_lgr))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "PRQpslhhXiLa",
        "outputId": "56e8baa5-c4a5-4d94-d5a7-43daab35d22f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of the model: 0.8812361759646105\n"
          ]
        }
      ],
      "source": [
        "# Logistic Regression model accuracy\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "acc_lgr_eval = MulticlassClassificationEvaluator(metricName='accuracy')\n",
        "acc_lgr = acc_lgr_eval.evaluate(test_lgr)\n",
        "print(\"Accuracy of the model: {}\".format(acc_lgr))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywhXgdXTXpxs"
      },
      "source": [
        "Linear SVC Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "rV4zGrpaXsYA"
      },
      "outputs": [],
      "source": [
        "# Linear SVC model\n",
        "from pyspark.ml.feature import CountVectorizer, IDF\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import LinearSVC\n",
        "lsvc = LinearSVC(maxIter=5)\n",
        "pipeline_lsvc = Pipeline(stages=[count_vec, idf, lsvc])\n",
        "\n",
        "model_lsvc = pipeline_lsvc.fit(training)\n",
        "test_lsvc = model_lsvc.transform(testing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "khI38-KOXv2v",
        "outputId": "e2d3f5a0-dd2f-4f90-d95b-747f17ee7347"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ROC of the model: 0.874372286344139\n"
          ]
        }
      ],
      "source": [
        "# Linear SVC model ROC\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "roc_lsvc_eval = BinaryClassificationEvaluator(rawPredictionCol='prediction', labelCol='label')\n",
        "roc_lsvc = roc_lsvc_eval.evaluate(test_lsvc)\n",
        "print(\"ROC of the model: {}\".format(roc_lsvc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "QrggwH5vXxP6",
        "outputId": "0cf23cfe-04f3-416f-d57f-813e8ad6edc7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of the model: 0.8741705578766281\n"
          ]
        }
      ],
      "source": [
        "# Linear SVC model accuracy\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "acc_lsvc_eval = MulticlassClassificationEvaluator(metricName='accuracy')\n",
        "acc_lsvc = acc_lsvc_eval.evaluate(test_lsvc)\n",
        "print(\"Accuracy of the model: {}\".format(acc_lsvc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NrWoPBFdAcJ"
      },
      "source": [
        "***Prediction of New Reviews By Model***\n",
        "\n",
        "Randomly choosen five reviews from the Kindle book The Brave Ones: A Memoir of Hope, Pride and Military Service, by Michael J.MacLeod. The suffixes \"_1\", \"_2\", ..., \"_5\" indicate the real overall review stars 1, 2, ..., 5.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0wcr43r6dE7F"
      },
      "outputs": [],
      "source": [
        "review_1 = [\"WOW!!! No words describe how bland this book is. It took me a lot to even pick up to read. I would definitely not recommend this book.\"]\n",
        "review_2 = [\"A first person account of the war in Afghanistan. It skipps around a lot and is like a never-ending news article. On the positive side, you do get a feel for what desert fighting is like from a soldiers point of view.\"]\n",
        "review_3 = [\"I liked the premise and most of the book. At the end parts I lost a little interest because I lost the thread of who was who. War is hell. MacLeod did his service unlike most of us.\"]\n",
        "review_4 = [\"Very informative first person account of the the daily life of a US Paratrooper. From training to deployment in combat situations in Afghanistan. Well worth the read and makes you really understand and appreciate their sacrifices\"]\n",
        "review_5 = [\"This is perhaps the best wrote book I have ever read. Articulate and thought provoking. Not just a riveting account of actual combat, but Michael was able to do what few before him have...captured the essence of what one feels as the battle unfolds. Perhaps most of all, I am grateful to call this author 'Fellow Warrior' Airborne all the way!!!\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Zt4enzM2dmEf"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.types import *\n",
        "schema = StructType([StructField(\"text\", StringType(), True)])\n",
        "\n",
        "text = [review_1, review_2, review_3, review_4, review_5]\n",
        "review_new = spark.createDataFrame(text, schema=schema)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "KNkXM8DZeOdQ",
        "outputId": "2c3f4ea7-7026-41bb-a79a-82e7921d3a4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+\n",
            "|                text|              tokens|\n",
            "+--------------------+--------------------+\n",
            "|WOW!!! No words d...|[wow, not_word, n...|\n",
            "|A first person ac...|[a, first, person...|\n",
            "|I liked the premi...|[i, liked, the, p...|\n",
            "|Very informative ...|[very, informativ...|\n",
            "|This is perhaps t...|[this, is, perhap...|\n",
            "+--------------------+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Data preprocessing\n",
        "review_new_tokens = review_new.withColumn('tokens', clean_udf(col('text')))\n",
        "review_new_tokens.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "BVTiNpIldsit",
        "outputId": "ea345776-6a44-47dc-e7eb-c2857d7a0e9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+----------+\n",
            "|                text|prediction|\n",
            "+--------------------+----------+\n",
            "|WOW!!! No words d...|       1.0|\n",
            "|A first person ac...|       1.0|\n",
            "|I liked the premi...|       1.0|\n",
            "|Very informative ...|       0.0|\n",
            "|This is perhaps t...|       0.0|\n",
            "+--------------------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Prediction using Linear SVC Model\n",
        "result = model_lsvc.transform(review_new_tokens)\n",
        "result.select('text', 'prediction').show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "rzclzya3v8gr",
        "outputId": "002d13da-549a-4251-be21-1f9a443df91b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+----------+\n",
            "|                text|prediction|\n",
            "+--------------------+----------+\n",
            "|WOW!!! No words d...|       1.0|\n",
            "|A first person ac...|       1.0|\n",
            "|I liked the premi...|       1.0|\n",
            "|Very informative ...|       0.0|\n",
            "|This is perhaps t...|       1.0|\n",
            "+--------------------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Prediction using Logistic Regression Model\n",
        "result = model_lgr.transform(review_new_tokens)\n",
        "result.select('text', 'prediction').show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5gcPby9-wMdk",
        "outputId": "9998d7b0-7e2c-4ab8-a1fb-79e8485aacb5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+----------+\n",
            "|                text|prediction|\n",
            "+--------------------+----------+\n",
            "|WOW!!! No words d...|       1.0|\n",
            "|A first person ac...|       1.0|\n",
            "|I liked the premi...|       1.0|\n",
            "|Very informative ...|       0.0|\n",
            "|This is perhaps t...|       1.0|\n",
            "+--------------------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Prediction using Naive Bayes Model\n",
        "result = model_nb.transform(review_new_tokens)\n",
        "result.select('text', 'prediction').show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "eiy5lGuxZCaM"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}